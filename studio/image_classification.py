from typing import Dict, Dict, Any, Optional, List, Annotated
from typing_extensions import TypedDict
from pydantic import BaseModel, Field
from enum import Enum
import operator
import base64
from io import BytesIO
from PIL import Image

from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnablePassthrough, RunnableConfig
from langchain_core.output_parsers import StrOutputParser

from langchain_openai import ChatOpenAI

from langgraph.graph import START, END, StateGraph
from langgraph.prebuilt import ToolNode


# Define the state which will be passed around
class ImageClassificationState(BaseModel):

    image: Optional[Any] = None # image data
    image_data: Optional[Dict[str, Any]] = None  # processed image data
    description: Optional[str] = None # description of the image, generated by the model
    history: List[Dict[str, Any]] = [] # history of the conversation (memory)
    sources: Optional[List[str]] = [] # list of sources found from webscraping
    classification: Optional[str] = None # classification of the image, generated by the model

def load_image(state: ImageClassificationState) -> Dict[str, Any]:

    if not state.image:
        return state
    
    try:
        image = Image.open(state.image)
        buffered = BytesIO()
        image.save(buffered, format=image.format or "JPEG")
        img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
        
        return ImageClassificationState(
            image=state.image,
            image_data={
                "success": True,
                "image_data": img_str,
                "width": image.width,
                "height": image.height,
                "format": image.format
            },
            description=state.description,
            history=state.history,
            sources=state.sources
        )
        
    except Exception as e:
        return ImageClassificationState(
            image=state.image,
            description=state.description,
            history=state.history + [{"role": "system", "content": f"Error: {str(e)}"}],
            sources=state.sources
        )


# Function to extract description from image using multimodal LLM (Openai GPT-4 vision for now) 
# This is the function which is the Node or the "Agent" in the graph.
def describe_image(state: ImageClassificationState) -> ImageClassificationState:

    if not state.image_data or not state.image_data.get("success", False):
        return state
    
    try:
        llm = ChatOpenAI(
            model="gpt-4o-mini", 
            temperature=0.2,
        )
        
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="You are an AI assistant that provides detailed descriptions of images."),
            HumanMessage(content=[
                {"type": "text", "text": "Please provide a detailed description of this image:"},
                {"type": "image_url", "image_url": {
                    "url": f"data:image/jpeg;base64,{state.image_data['image_data']}"
                }}
            ])
        ])
        
        chain = prompt | llm | StrOutputParser()
        description = chain.invoke({})
        
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=description,
            history=state.history + [{"role": "system", "content": "Image processed and described"}],
            sources=state.sources
        )
        
    except Exception as e:
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=state.description,
            history=state.history + [{"role": "system", "content": f"Error: {str(e)}"}],
            sources=state.sources
        )
 

def create_image_classification_graph():
    workflow = StateGraph(ImageClassificationState)
    
    workflow.add_node("initialize", lambda state: ImageClassificationState(**state.model_dump()))
    workflow.add_node("load_image", load_image)
    workflow.add_node("describe_image", describe_image)
    
    # Define the edges between nodes
    workflow.add_edge(START, "initialize")
    workflow.add_edge("initialize", "load_image")
    workflow.add_edge("load_image", "describe_image")
    workflow.add_edge("describe_image", END)
    
    # Compile the graph
    return workflow.compile()

# Create the graph, to launche using 'langgraph dev'
graph = create_image_classification_graph()
