from typing import Dict, Any, Optional, List
from typing_extensions import TypedDict
from pydantic import BaseModel, Field
from enum import Enum
import operator
import base64
from io import BytesIO
from PIL import Image
import re

from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnablePassthrough, RunnableConfig
from langchain_core.output_parsers import StrOutputParser

# Import Ollama integration
from langchain_ollama import OllamaLLM
from langchain_core.messages import HumanMessage, AIMessage

# Import search tools
from langchain_community.tools import TavilySearchResults
from langchain_google_community import GoogleSearchAPIWrapper

from langgraph.graph import START, END, StateGraph
from langgraph.prebuilt import ToolNode


# Define the state which will be passed around
class ImageClassificationState(BaseModel):
    image: Optional[Any] = None # image data
    image_data: Optional[Dict[str, Any]] = None  # processed image data
    description: Optional[str] = None # description of the image, generated by the model
    search_query: Optional[str] = None # optimized search query based on the description
    history: List[Dict[str, Any]] = [] # history of the conversation (memory)
    sources: List[str] = [] # list of sources found from webscraping
    classification: Optional[str] = None # classification of the image (REAL or FAKE)
    confidence: Optional[int] = None # confidence score (0-100)
    explanation: Optional[str] = None # explanation for the classification

def load_image(state: ImageClassificationState) -> ImageClassificationState:
    if not state.image:
        return state
    
    try:
        image = Image.open(state.image)
        buffered = BytesIO()
        image.save(buffered, format=image.format or "JPEG")
        img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
        
        return ImageClassificationState(
            image=state.image,
            image_data={
                "success": True,
                "image_data": img_str,
                "width": image.width,
                "height": image.height,
                "format": image.format
            },
            description=state.description,
            history=state.history,
            sources=state.sources
        )
        
    except Exception as e:
        return ImageClassificationState(
            image=state.image,
            description=state.description,
            history=state.history + [{"role": "system", "content": f"Error: {str(e)}"}],
            sources=state.sources
        )


# Function to extract description from image using multimodal LLM (Ollama bakllava) 
def describe_image(state: ImageClassificationState) -> ImageClassificationState:
    if not state.image_data or not state.image_data.get("success", False):
        return state
    
    try:
        # Initialize Ollama with llava model
        llm = OllamaLLM(model="llava")
        
        # Get the base64 image from state
        image_base64 = state.image_data['image_data']
        
        # Create prompt template for image description
        template = """You are an AI assistant that provides detailed descriptions of images.
        Focus on key elements that can be verified:
        - People or notable figures in the image and their names
        - Location and setting
        - Any visible text or signs
        - Events or activities depicted
        - Distinctive objects or landmarks
        - Approximate time period or date indicators
        
        Please provide a detailed factual description of this image:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        # Bind the image to the LLM
        llm_with_image = llm.bind(images=[image_base64])
        
        # Create the chain
        chain = prompt | llm_with_image
        
        # Get response from the model
        description = chain.invoke({})
        
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=description,
            history=state.history + [{"role": "system", "content": "Image processed and described"}],
            sources=state.sources
        )
        
    except Exception as e:
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=state.description,
            history=state.history + [{"role": "system", "content": f"Error in describe_image: {str(e)}"}],
            sources=state.sources
        )

# Function to create an optimized search query from the image description
def optimize_search_query(state: ImageClassificationState) -> ImageClassificationState:
    if not state.description:
        return state
    
    try:
        # Use Ollama for text-only task
        llm = OllamaLLM(model="llama3")
        
        # Simple prompt for search query optimization
        prompt = f"""Given this image description, create a concise search query that will be used to verify if the image represents a real event.
        The query should focus on the most distinctive and verifiable elements, include names, locations, or dates if present, and be under 30 words.
        
        IMAGE DESCRIPTION:
        {state.description}
        
        SEARCH QUERY:"""
        
        search_query = llm.invoke(prompt).strip()
        
        # Clean up the query (remove any additional text the model might add)
        if len(search_query.split()) > 30:
            search_query = " ".join(search_query.split()[:30])
        
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=state.description,
            search_query=search_query,
            history=state.history + [{"role": "system", "content": f"Created search query: {search_query}"}],
            sources=state.sources
        )
        
    except Exception as e:
        # Fallback to using first part of description if there's an error
        search_query = " ".join(state.description.split()[:30])
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=state.description,
            search_query=search_query,
            history=state.history + [{"role": "system", "content": f"Error creating search query: {str(e)}. Using fallback."}],
            sources=state.sources
        )

# Function to perform web scraping using both Tavily and Google Search
def webscrape_content(state: ImageClassificationState) -> ImageClassificationState:
    if not state.search_query and not state.description:
        return state
    
    try:

        query = state.search_query if state.search_query else state.description
        tavily_search = TavilySearchResults(max_results=3)

        tavily_results = tavily_search.invoke({
            "query": f"Verify authenticity of: {query}",
            "max_results": 3
        })
        
        sources = []
        
        for result in tavily_results:
            if "url" in result and result["url"] not in sources:
                sources.append(result["url"])
        
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=state.description,
            search_query=state.search_query,
            history=state.history + [{"role": "system", "content": f"Found {len(sources)} sources through web scraping"}],
            sources=sources
        )
        
    except Exception as e:
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=state.description,
            search_query=state.search_query,
            history=state.history + [{"role": "system", "content": f"Web scraping error: {str(e)}"}],
            sources=state.sources
        )

# Function to classify the image as real or fake based on search results
def classify_image(state: ImageClassificationState) -> ImageClassificationState:
    if not state.description or not state.sources:
        return state
    
    try:
        llm = OllamaLLM(model="llama3")
        
        sources_text = "\n".join([f"- {source}" for source in state.sources])
        
        prompt = f"""You are an image verification specialist. Determine if this image represents a real event or fake news.

IMAGE DESCRIPTION:
{state.description}

SEARCH QUERY USED:
{state.search_query if state.search_query else "N/A"}

WEB SOURCES:
{sources_text}

Analyze if the sources confirm or refute the authenticity of what's described in the image.
Consider:
- If credible sources mention the event/scene described
- Consistency between image description and information from reliable sources
- Evidence of manipulation or misrepresentation
- Presence in fact-checking websites

Based only on this information, provide your verdict in this exact format:
CLASSIFICATION: [REAL or FAKE]
CONFIDENCE: [0-100]
EXPLANATION: [Your detailed explanation with references to specific sources]"""

        analysis = llm.invoke(prompt)
        
        # Parse the analysis to extract classification, confidence, and explanation
        classification = "UNKNOWN"
        confidence = 0
        explanation = analysis
        
        # Extract classification
        classification_match = re.search(r'CLASSIFICATION:\s*(REAL|FAKE)', analysis, re.IGNORECASE)
        if classification_match:
            classification = classification_match.group(1).upper()
        
        # Extract confidence
        confidence_match = re.search(r'CONFIDENCE:\s*(\d+)', analysis)
        if confidence_match:
            confidence = int(confidence_match.group(1))
            # Ensure confidence is within valid range
            confidence = max(0, min(100, confidence))
        
        # Extract explanation
        explanation_match = re.search(r'EXPLANATION:(.*)', analysis, re.DOTALL)
        if explanation_match:
            explanation = explanation_match.group(1).strip()
        
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=state.description,
            search_query=state.search_query,
            history=state.history + [{"role": "system", "content": f"Image classified as {classification} with {confidence}% confidence"}],
            sources=state.sources,
            classification=classification,
            confidence=confidence,
            explanation=explanation
        )
        
    except Exception as e:
        return ImageClassificationState(
            image=state.image,
            image_data=state.image_data,
            description=state.description,
            search_query=state.search_query,
            history=state.history + [{"role": "system", "content": f"Classification error: {str(e)}"}],
            sources=state.sources
        )

def create_image_classification_graph():
    workflow = StateGraph(ImageClassificationState)
    
    # Add nodes
    workflow.add_node("initialize", lambda state: ImageClassificationState(**state.model_dump()))
    workflow.add_node("load_image", load_image)
    workflow.add_node("describe_image", describe_image)
    workflow.add_node("optimize_search_query", optimize_search_query)
    workflow.add_node("webscrape_content", webscrape_content)
    workflow.add_node("classify_image", classify_image)
    
    # Define the edges between nodes (sequential flow)
    workflow.add_edge(START, "initialize")
    workflow.add_edge("initialize", "load_image")
    workflow.add_edge("load_image", "describe_image")
    workflow.add_edge("describe_image", "optimize_search_query")
    workflow.add_edge("optimize_search_query", "webscrape_content")
    workflow.add_edge("webscrape_content", "classify_image")
    workflow.add_edge("classify_image", END)
    
    # Compile the graph
    return workflow.compile()

# Create the graph, to launch using 'langgraph dev'
graph = create_image_classification_graph()